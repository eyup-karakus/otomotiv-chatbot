import streamlit as st
import pandas as pd
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from llama_cpp import Llama

# .env'den API anahtarÄ±nÄ± al
load_dotenv()
gemini_api_key = os.getenv("GEMINI_API_KEY")

# Gemini modeli tanÄ±mla
llm_gemini = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=gemini_api_key)

# LLaMA modelini baÅŸlat (lokalde)
llama_model = Llama(
    model_path="models/llama/capybarahermes-2.5-mistral-7b.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=4,
    verbose=False
)

def ask_llama(prompt: str) -> str:
    response = llama_model(prompt, max_tokens=512, stop=["</s>"])
    return response["choices"][0]["text"].strip()

# FAISS + model yÃ¼kleme
@st.cache_resource
def load_model_and_data():
    model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    index = faiss.read_index("data/faq_index.faiss")
    mapping = pd.read_csv("data/faq_mapping.csv")
    return model, index, mapping

model, index, mapping = load_model_and_data()

# GeÃ§miÅŸi saklamak iÃ§in session state
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Sidebar
with st.sidebar:
    st.markdown("# ğŸš— Otomotiv AsistanÄ±")
    st.markdown("### âš™ï¸ Ayarlar")
    model_choice = st.selectbox("Modelinizi SeÃ§iniz:", ["Gemini", "LLaMA (local)"])
    if st.button("ğŸ§¹ GeÃ§miÅŸi Temizle"):
        st.session_state.chat_history = []

# Ana BaÅŸlÄ±k
st.title("ğŸš— Otomotiv SÃ¼reÃ§leri Chatbot")
st.markdown("PPAP, APQP, Homologasyon ve benzeri otomotiv sÃ¼reÃ§leri hakkÄ±nda sorularÄ±nÄ±zÄ± sorun.")

# KullanÄ±cÄ± giriÅŸi
user_input = st.text_input("âœï¸ Soru sorun", placeholder="Otomotiv sÃ¼reÃ§leri hakkÄ±nda her ÅŸey :)")
if st.button("GÃ¶nder â¤") and user_input:
    # Embed ve FAISS arama
    embedded = model.encode([user_input])
    D, I = index.search(np.array(embedded), k=1)
    matched_row = mapping.iloc[I[0][0]]
    closest_q = matched_row["user_message"]
    intent = matched_row["intent"]
    similarity_score = 100 - round(D[0][0], 2)  # KÃ¼Ã§Ã¼k mesafe = yÃ¼ksek benzerlik

    # Ortak prompt
    prompt_text = f"""
    Sen otomotiv sektÃ¶rÃ¼nde Ã§alÄ±ÅŸanlara bilgi veren teknik bir asistansÄ±n.
    KullanÄ±cÄ± sana ÅŸunu sordu:
    "{user_input}"

    AÅŸaÄŸÄ±da sistemde buna en yakÄ±n eÅŸleÅŸen veri var:
    "{closest_q}"

    Bu kaynaÄŸa dayanarak aÃ§Ä±k ve doÄŸru bir cevap Ã¼ret.
    """

    # YanÄ±t Ã¼retimi
    if model_choice == "Gemini":
        prompt = ChatPromptTemplate.from_template(prompt_text)
        final_prompt = prompt.format_messages()
        response = llm_gemini.invoke(final_prompt)
        answer = response.content
    else:
        answer = ask_llama(prompt_text)

    # GeÃ§miÅŸe ekle
    st.session_state.chat_history.append({
        "soru": user_input,
        "cevap": answer,
        "intent": intent,
        "eslesen_soru": closest_q,
        "benzerlik": similarity_score
    })

# Sohbet geÃ§miÅŸi
for idx, chat in enumerate(reversed(st.session_state.chat_history)):
    with st.expander(f"ğŸ—¨ï¸ {chat['soru']}", expanded=False):
        st.markdown(f"**ğŸ” EÅŸleÅŸen Soru:** {chat['eslesen_soru']}")
        st.markdown(f"**ğŸ¯ Intent:** {chat['intent']}")
        st.markdown(f"**ğŸ“Š Benzerlik Skoru:** %{chat['benzerlik']}")
        st.markdown(f"**ğŸ¤– YanÄ±t:** {chat['cevap']}")